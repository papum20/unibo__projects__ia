# -*- coding: utf-8 -*-
"""project.ipynb

Automatically generated by Colab.

Original file is located at
	https://colab.research.google.com/drive/1K7v21oho6bkLGGm9fXiwG35X80FNMy4e

# Anomaly Detection in Cifar10

40 immagini deteriorate sono state aggiunte a una versione modificata del dataset cifar10.

TROVATELE!!

Istruzioni per scaricare il dataset sono date più in basso.
L'analisi deve fare riferimento solo al dataset fornito (non potete utilizzare in alcun modo Cifar10 originale).

## Cosa consegnare

Il risultato deve essere fornito sotto forma di **una lista di 40 indici** relativi agli outliers identificati.

Non potete restituire liste più lunghe. In tal caso, verrano troncate ai primi 40 elementi.

Il notebook deve spiegare la metodologia adottata e contenere le relative procedure. La metodologia deve essere automatica e non può prevedere nessuna supervisione umana.

## Dataset Downloading
"""

import numpy as np
import gdown

"""The following line should create in you local dicrectory a file named dataset.npy"""

# Use this line for Colab instead
#!gdown 1lccprYS7eWQBsLBsZS9J8qnETzRHBNZa
import os
if not os.path.exists('../dataset.npy'):
	gdown.download('https://drive.google.com/uc?id=1lccprYS7eWQBsLBsZS9J8qnETzRHBNZa', '../dataset.npy', quiet=False)

dataset = np.load('../dataset.npy',allow_pickle=True)

"""The dataset has shape (59900, 32, 32, 3)
No need to split it into train, validation and test.
"""

print(dataset.shape)

"""**IMPORTANTE**

Non riordinate il dataset caricato.
La lista di indici che restituite deve fare riferimento ad esso.


**Buon lavoro!!**

# Implementazione

Definizioni iniziali:
"""

import matplotlib.pyplot as plt

# True if running on Google Colab, False if in local
IN_COLAB = False

# True to allow loading saved weights
LOAD_WEIGHTS = False

INPUT_SIZE = 32*32*3	# 3072

if IN_COLAB:
	from google.colab import drive
	mount_path = '/content/drive'
	drive.mount(mount_path)
	save_path = mount_path +'/MyDrive/Colab Notebooks/res'
	PATH_WEIGHTS_AUTOENCODER	= f"{save_path}/weights_autoencoder.weights.h5"
	PATH_WEIGHTS_CONV			= f"{save_path}/weights_conv.weights.h5"
else:
	PATH_WEIGHTS_AUTOENCODER	= "weights_autoencoder.weights.h5"
	PATH_WEIGHTS_CONV			= "weights_conv.weights.h5"

PLOT_SHOW		= False
PLOT_COUNTER	= 0
def PATH_PLOT(name):
	global PLOT_COUNTER
	PLOT_COUNTER += 1
	return f"plot_{name}_{PLOT_COUNTER - 1}.png"


def img_show(img):
	plt.imshow(img, cmap="gray")
	plt.show()

def plot_images_horizontally(images, num_images, cmap='gray', name='horizontal'):
	plt.figure(figsize=(20, 20))
	for i in range(num_images):
		plt.subplot(1, num_images, i+1)
		plt.imshow(images[i], cmap=cmap)
		plt.axis('off')
	plt.savefig( PATH_PLOT(name) )
	if PLOT_SHOW:
		plt.show()

def plot_images(images, figsize, cmap='gray', name='grid'):
	scaler = 10
	plt.figure(figsize=(figsize[0]*scaler, figsize[1]*scaler))
	for i in range(min(figsize[0]*figsize[1], len(images))):
		plt.subplot(figsize[0], figsize[1], i+1)
		plt.imshow(images[i], cmap=cmap)
		plt.axis('off')
	plt.savefig( PATH_PLOT(name) )
	if PLOT_SHOW:
		plt.show()

def plot_hist(data, bins=50, name='hist'):
	plt.hist(data, bins=bins)
	plt.xlabel('Reconstruction error')
	plt.ylabel('No of examples')
	plt.title('Distribution of reconstruction errors')
	plt.savefig( PATH_PLOT(name) )
	if PLOT_SHOW:
		plt.show()


def plot_training_history(history, num_epochs, name='history'):

	training_loss = history.history['loss']
	training_metrics = history.history['accuracy']

	epochs = range(1, num_epochs + 1)
	plt.figure(figsize=(12, 4))

	plt.subplot(1, 2, 1)
	plt.plot(epochs, training_loss, label='Training Loss')
	plt.title('Loss')
	plt.xlabel('Epochs')
	plt.legend()

	plt.subplot(1, 2, 2)
	plt.plot(epochs, training_metrics, label='Training Accuracy')
	plt.title('Accuracy')
	plt.xlabel('Epochs')
	plt.legend()

	plt.tight_layout()
	plt.savefig( PATH_PLOT(name) )
	if PLOT_SHOW:
		plt.show()

def plot_clusters(x_axis,y_axis, y_predict, title, name='clusters'):
	plt.figure(figsize=(12, 4))
	plt.scatter(x_axis, y_axis, c=y_predict, cmap='viridis')
	plt.title(title)
	plt.savefig( PATH_PLOT(name) )
	if PLOT_SHOW:
		plt.show()

def plot_hist(data, label_x, label_y, title, bins=50, log_scale=True, name='hist'):
	plt.figure(figsize=(12, 4))
	plt.hist(data, bins=bins)
	if log_scale:
		plt.yscale('log')
	plt.xlabel(label_x)
	plt.ylabel(label_y)
	plt.title(title)
	plt.savefig( PATH_PLOT(name) )
	if PLOT_SHOW:
		plt.show()

def plot_hist_err_reconstruction(data, bins=50, log_scale=True, name='hist'):
	plot_hist(data, 'Reconstruction error', 'No of examples', 'Distribution of reconstruction errors', bins=bins, log_scale=log_scale, name=name)

def plot_scatter(x, y, label, label_x, label_y, name='scatter'):
	plt.figure(figsize=(12, 4))
	plt.scatter(x, y, label=label)
	plt.legend()
	plt.xlabel(label_x)
	plt.ylabel(label_y)
	plt.savefig( PATH_PLOT(name) )
	if PLOT_SHOW:
		plt.show()

def plot_scatter_err_reconstruction(errors, name='scatter'):
	indices = range(len(errors))
	plot_scatter(indices, errors, 'Reconstruction error', 'Index', 'Reconstruction error', name=name)


def plot_training_history(history, num_epochs, name='history'):

	training_loss = history.history['loss']
	training_metrics = history.history['accuracy']

	epochs = range(1, num_epochs + 1)
	plt.figure(figsize=(12, 4))

	plt.subplot(1, 2, 1)
	plt.plot(epochs, training_loss, label='Training Loss')
	plt.title('Loss')
	plt.xlabel('Epochs')
	plt.legend()

	plt.subplot(1, 2, 2)
	plt.plot(epochs, training_metrics, label='Training Accuracy')
	plt.title('Accuracy')
	plt.xlabel('Epochs')
	plt.legend()

	plt.tight_layout()
	plt.savefig( PATH_PLOT(name) )
	if PLOT_SHOW:
		plt.show()



"""
## Logistic Regression
"""

import numpy as np
import matplotlib.pyplot as plt
from sklearn.decomposition import PCA
from sklearn.datasets import load_iris
from sklearn.preprocessing import StandardScaler

X = (dataset / 255.0).reshape(dataset.shape[0], -1)
print(f"{X.shape = }")


# Standardizzazione dei dati (mu 0, std 1)
X_std = StandardScaler().fit_transform(X)
print(f"{X_std.shape = }")

#Usiamo la pca per ridurre le features

n_comp = 2
pca = PCA(n_components=n_comp)

# Addestramento della PCA sui dati standardizzati
X_reduced = pca.fit_transform(X_std)

print(f"Dataset ridotto shape: {X_reduced.shape}")
print(f"Numero di esempi: {X_reduced.shape[0]}")
print(f"Numero di features (Input Dimensionality): {X_reduced.shape[1]}")

X = X_reduced


#Prendiamo i 3 dataset della volta scorsa ed effettuiamo il clustering dei punti.

from sklearn.model_selection import cross_val_predict, cross_val_score
from sklearn.metrics import confusion_matrix
from sklearn.model_selection import train_test_split
import numpy as np
from sklearn import preprocessing



n_samples = 1000
n_features = 2
n_classes = 2
seed = 42
noise_factor = 0.1
np.random.seed(seed)

## Data preprocessing
# Creiamo un dataset di esempio con due classi in proporzione diversa
scaler = preprocessing.MinMaxScaler() # range [0,1]
scaler.fit(X)
X = scaler.transform(X)



import time
import matplotlib

# Example settings
n_samples = len(X)
print(f"{n_samples = }")
n_outliers = 40
outliers_fraction = float(n_outliers) / n_samples
n_inliers = n_samples - n_outliers
rng = np.random.RandomState(42)

matplotlib.rcParams["contour.negative_linestyle"] = "solid"



from sklearn.ensemble import IsolationForest
from sklearn.svm import OneClassSVM
from sklearn.neighbors import LocalOutlierFactor
from sklearn.covariance import EllipticEnvelope
from sklearn.metrics import mean_squared_error

#Ho un punto e voglio classificarlo come simile o non simile ai suoi vicini
#Costruisco degli alberi decisionali per farlo e vedo quanto sono profondi
#le anomalie hanno alberi poco profondi perchè è diverso dai suoi vicini.
isolation_forest = IsolationForest(contamination=outliers_fraction)

# One-Class SVM (Support Vector Machine)
oc_svm = OneClassSVM(nu=outliers_fraction,  kernel="rbf", gamma=0.1)  # Imposta il parametro nu, che controlla la percentuale di punti normali

print("DONE SVM")

lof = LocalOutlierFactor(n_neighbors=35, contamination=outliers_fraction, novelty=True )  # Imposta novelty=True per il rilevamento delle anomalie
# LOF è un valore calcolato sulla base della densità locale di un punto dato, e misura quanto un punto è significativamente diverso (e quindi è un outlier) rispetto ai suoi vicini.

print("DONE LOF")

# Elliptic Envelope presuppone che i dati abbiano distribuzione gaussiana e guarda alla loro covarianza (crea solo delle ellissi)
elliptic = EllipticEnvelope(contamination=outliers_fraction, random_state=42) # Imposta la percentuale di contaminazione

print("DONE Elliptic")


anomaly_algorithms=[
    ("Robust covariance",elliptic ),
    ("Isolation Forest",isolation_forest ),
    ("One-Class SVM",oc_svm ),
    ("Local Outlier Factor",lof )
]
xx, yy = np.meshgrid(np.linspace(-7, 7, 150), np.linspace(-7, 7, 150))

plt.figure(figsize=(len(anomaly_algorithms) * 4, 4))
plt.subplots_adjust(
    left=0.02, right=0.98, bottom=0.01, top=0.96, wspace=0.05, hspace=0.2
)

plot_num = 1

for name, algorithm in anomaly_algorithms:
	t0 = time.time()
	if name == "Local Outlier Factor" and algorithm.novelty:
		y_pred =  algorithm.fit(X).predict(X)
	else:
		y_pred =  algorithm.fit_predict(X)

	"""
	error_reconstruction = np.mean(np.power(X - y_pred, 2), axis=1)

	plot_hist_err_reconstruction(error_reconstruction, name=f"{name}_hist")
	plot_scatter_err_reconstruction(error_reconstruction, name=f"{name}_scatter")
	
	anomalies = error_reconstruction.argsort()[-40:]
	anomalies = sorted(anomalies)

	print(anomalies)
	anomalies_img = []
	for i in anomalies:
		anomalies_img.append(dataset[i])
	plot_images(anomalies_img, (8,5), name=f"{name}")
	"""

	t1 = time.time()
	plt.subplot(1, len(anomaly_algorithms), plot_num)
	plt.title(name, size=18)

	Z = -algorithm.decision_function(np.c_[xx.ravel(), yy.ravel()]).reshape(xx.shape)
	plt.contourf(xx, yy, Z, cmap=plt.cm.Blues, alpha=0.6)

	colors = np.array(["#377eb8", "#ff7f00"])
	plt.scatter(X[:, 0], X[:, 1], s=10, color=colors[(y_pred + 1) // 2])

	plt.xlim(-7, 7)
	plt.ylim(-7, 7)
	plt.xticks(())
	plt.yticks(())
	plt.text(
		0.99,
		0.01,
		("%.2fs" % (t1 - t0)).lstrip("0"),
		transform=plt.gca().transAxes,
		size=15,
		horizontalalignment="right",
	)
	plot_num += 1

plt.show()