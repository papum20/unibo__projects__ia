# -*- coding: utf-8 -*-
"""project.ipynb

Automatically generated by Colab.

Original file is located at
	https://colab.research.google.com/drive/1K7v21oho6bkLGGm9fXiwG35X80FNMy4e

# Anomaly Detection in Cifar10

40 immagini deteriorate sono state aggiunte a una versione modificata del dataset cifar10.

TROVATELE!!

Istruzioni per scaricare il dataset sono date più in basso.
L'analisi deve fare riferimento solo al dataset fornito (non potete utilizzare in alcun modo Cifar10 originale).

## Cosa consegnare

Il risultato deve essere fornito sotto forma di **una lista di 40 indici** relativi agli outliers identificati.

Non potete restituire liste più lunghe. In tal caso, verrano troncate ai primi 40 elementi.

Il notebook deve spiegare la metodologia adottata e contenere le relative procedure. La metodologia deve essere automatica e non può prevedere nessuna supervisione umana.

## Dataset Downloading
"""

import numpy as np
import gdown

"""The following line should create in you local dicrectory a file named dataset.npy"""

# Use this line for Colab instead
#!gdown 1lccprYS7eWQBsLBsZS9J8qnETzRHBNZa
import os
if not os.path.exists('dataset.npy'):
	gdown.download('https://drive.google.com/uc?id=1lccprYS7eWQBsLBsZS9J8qnETzRHBNZa', 'dataset.npy', quiet=False)

dataset = np.load('dataset.npy',allow_pickle=True)

"""The dataset has shape (59900, 32, 32, 3)
No need to split it into train, validation and test.
"""

print(dataset.shape)

"""**IMPORTANTE**

Non riordinate il dataset caricato.
La lista di indici che restituite deve fare riferimento ad esso.


**Buon lavoro!!**

# Implementazione

Definizioni iniziali:
"""

import matplotlib.pyplot as plt
import time

# True if running on Google Colab, False if in local
IN_COLAB = False

# True to allow loading saved weights
LOAD_WEIGHTS = True

EXEC_DENSE = False
TIMES_TO_FILE = True

INPUT_SIZE = 32*32*3	# 3072

if IN_COLAB:
	from google.colab import drive
	mount_path = '/content/drive'
	drive.mount(mount_path)
	save_path = mount_path +'/MyDrive/Colab Notebooks/res'
	PATH_WEIGHTS_AUTOENCODER	= f"{save_path}/weights_autoencoder.weights.h5"
	PATH_WEIGHTS_CONV			= f"{save_path}/weights_conv.weights.h5"
else:
	PATH_WEIGHTS_AUTOENCODER	= "weights_autoencoder.weights.h5"
	PATH_WEIGHTS_CONV			= "weights_conv.weights.h5"

PATH_TIMES = "times.txt"

PLOT_SHOW		= False
PLOT_COUNTER	= 0
def PATH_PLOT(name):
	global PLOT_COUNTER
	PLOT_COUNTER += 1
	return f"plot_{name}_{PLOT_COUNTER - 1}.png"

def PATH_WEIGHTS(is_conv: bool, epochs_n: int, batch_size: int, filters: tuple[int,int]):
	if is_conv:
		return f"{PATH_WEIGHTS_CONV.split('.weights.h5')[0]}_{epochs_n}_{batch_size}_{filters[0]}_{filters[1]}.weights.h5"
	else:
		return f"{PATH_WEIGHTS_AUTOENCODER.split('.weights.h5')[0]}_{epochs_n}_{batch_size}_{filters[0]}_{filters[1]}.weights.h5"

def img_show(img):
	plt.imshow(img, cmap="gray")
	plt.show()

def plot_images_horizontally(images, num_images, cmap='gray', name='horizontal'):
	plt.figure(figsize=(20, 20))
	for i in range(num_images):
		plt.subplot(1, num_images, i+1)
		plt.imshow(images[i], cmap=cmap)
		plt.axis('off')
	plt.savefig( PATH_PLOT(name) )
	if PLOT_SHOW:
		plt.show()

def plot_images(images, figsize, cmap='gray', name='grid'):
	scaler = 10
	plt.figure(figsize=(figsize[0]*scaler, figsize[1]*scaler))
	for i in range(min(figsize[0]*figsize[1], len(images))):
		plt.subplot(figsize[0], figsize[1], i+1)
		plt.imshow(images[i], cmap=cmap)
		plt.axis('off')
	plt.savefig( PATH_PLOT(name) )
	if PLOT_SHOW:
		plt.show()

def plot_hist(data, label_x, label_y, title, bins=50, log_scale=True, name='hist'):
	plt.figure(figsize=(12, 4))
	plt.hist(data, bins=bins)
	if log_scale:
		plt.yscale('log')
	plt.xlabel(label_x)
	plt.ylabel(label_y)
	plt.title(title)
	plt.savefig( PATH_PLOT(name) )
	if PLOT_SHOW:
		plt.show()

def plot_hist_err_reconstruction(data, bins=50, log_scale=True, name='hist'):
	plot_hist(data, 'Reconstruction error', 'No of examples', 'Distribution of reconstruction errors', bins=bins, log_scale=log_scale, name=name)

def plot_scatter(x, y, label, label_x, label_y, name='scatter'):
	plt.figure(figsize=(12, 4))
	plt.scatter(x, y, label=label)
	plt.legend()
	plt.xlabel(label_x)
	plt.ylabel(label_y)
	plt.savefig( PATH_PLOT(name) )
	if PLOT_SHOW:
		plt.show()

def plot_scatter_err_reconstruction(errors, name='scatter'):
	indices = range(len(errors))
	plot_scatter(indices, errors, 'Reconstruction error', 'Index', 'Reconstruction error', name=name)


def plot_training_history(history, num_epochs, name='history'):

	training_loss = history.history['loss']
	training_metrics = history.history['accuracy']

	epochs = range(1, num_epochs + 1)
	plt.figure(figsize=(12, 4))

	plt.subplot(1, 2, 1)
	plt.plot(epochs, training_loss, label='Training Loss')
	plt.title('Loss')
	plt.xlabel('Epochs')
	plt.legend()

	plt.subplot(1, 2, 2)
	plt.plot(epochs, training_metrics, label='Training Accuracy')
	plt.title('Accuracy')
	plt.xlabel('Epochs')
	plt.legend()

	plt.tight_layout()
	plt.savefig( PATH_PLOT(name) )
	if PLOT_SHOW:
		plt.show()


#"""Gray scaling:"""
#
#x_gray = np.mean( dataset, axis=-1)
#
#print(f"{x_gray.shape = }")
#plot_images_horizontally(dataset[:10], 10, name="first10")
#plot_images_horizontally(x_gray[:10], 10, name="first10_gray")
#
"""Flattening:"""



"""Model (autoencoder):"""

# hypermarameters

BATCH_SIZE	= 256
N_EPOCHS	= 50
FILTERS	= (16,16)
CONV_KERNEL_SIZE	= (1,1)
CONV_STRIDE			= (1,1)
MAX_POOLING_KERNEL_SIZE	= (2,2)
MAX_POOLING_STRIDE		= (2,2)

from keras.models import Model
from keras.layers import Input, Dense
from keras.layers import Conv2D, MaxPooling2D, UpSampling2D


if EXEC_DENSE:

	ds_preprocessed = (dataset / 255.0).reshape(dataset.shape[0], -1)

	# Define the autoencoder
	input_img = Input(shape=(INPUT_SIZE,), name='input')
	encoded = Dense(128, activation='relu', name='encoded')(input_img)
	decoded = Dense(INPUT_SIZE, activation='sigmoid', name='decoded')(encoded)

	autoencoder = Model(input_img, decoded)
	autoencoder.summary()
	autoencoder.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

		
	for N_EPOCHS in (20, 40, 70):

		# Train
		try:
			if LOAD_WEIGHTS:
				autoencoder.load_weights(PATH_WEIGHTS(False, N_EPOCHS, BATCH_SIZE, FILTERS))
				print("Loaded weights for AUTOENCODER")
			else:
				raise Exception
		except Exception as e:
			print(f"Loading AUTOENCODER Error: {e}")
			print("Didn't load weights, training AUTOENCODER")
			history = autoencoder.fit(ds_preprocessed, ds_preprocessed, epochs=N_EPOCHS, batch_size=BATCH_SIZE, shuffle=True)
			if LOAD_WEIGHTS:
				autoencoder.save_weights(PATH_WEIGHTS(False, N_EPOCHS, BATCH_SIZE, FILTERS))
			print("Training done, weights saved")
			plot_training_history(history, N_EPOCHS, name=f"autoencoder{N_EPOCHS}_history")


		# Calculate the reconstruction error for each image
		error_reconstruction = np.mean(np.power(ds_preprocessed - autoencoder.predict(ds_preprocessed), 2), axis=1)

		plot_hist_err_reconstruction(error_reconstruction, name=f"autoencoder{N_EPOCHS}_hist")
		plot_scatter_err_reconstruction(error_reconstruction, name=f"autoencoder{N_EPOCHS}_scatter")

		# Identify the 40 images with the highest reconstruction error as anomalies
		anomalies = error_reconstruction.argsort()[-40:]
		anomalies = sorted(anomalies)

		print(anomalies)
		anomalies_img = []
		for i in anomalies:
			anomalies_img.append(dataset[i])
		plot_images(anomalies_img, (8,5), name=f"autoencoder{N_EPOCHS}")

"""not flattened (convolutional):"""

ds_preprocessed = dataset / 255.0

# Define the convolutional autoencoder
input_img = Input(shape=(32, 32, 3), name="input")
x = Conv2D(FILTERS[0], (3, 3), activation='relu', padding='same', name='encoded_conv2d_1')(input_img)
x = MaxPooling2D((2, 2), padding='same', name='encoded_maxPooling2d_1')(x)
x = Conv2D(FILTERS[1], (3, 3), activation='relu', padding='same', name='encoded_conv2d_2')(x)
encoded = MaxPooling2D((2, 2), padding='same', name='encoded_maxPooling2d_2')(x)

x = Conv2D(FILTERS[1], (3, 3), activation='relu', padding='same', name='decoded_conv2d_1')(encoded)
x = UpSampling2D((2, 2), name='decoded_upSampling2d_1')(x)
x = Conv2D(FILTERS[0], (3, 3), activation='relu', padding='same', name='decoded_conv2d_2')(x)
x = UpSampling2D((2, 2), name='decoded_upSampling2d_2')(x)
decoded = Conv2D(3, (3, 3), activation='sigmoid', padding='same', name='decoded_conv2d_3')(x)

autoencoder = Model(input_img, decoded)
autoencoder.summary()
autoencoder.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])



# Train
try:
	if LOAD_WEIGHTS:
		autoencoder.load_weights(PATH_WEIGHTS(True, N_EPOCHS, BATCH_SIZE, FILTERS))
		print("Loaded weights for CONVOLUTIONAL")
	else:
		raise Exception
except Exception as e:
	print(f"Loading CONVOLUTIONAL Error: {e}")
	print("Didn't load weights, training CONVOLUTIONAL")

	time_start = time.time()
	history = autoencoder.fit(ds_preprocessed, ds_preprocessed, epochs=N_EPOCHS, batch_size=BATCH_SIZE, shuffle=True)
	time_end = time.time()
	if(TIMES_TO_FILE):
		with open(PATH_TIMES, "a") as f:
			f.write(f"conv\t{N_EPOCHS}\t{BATCH_SIZE}\t{FILTERS}\t: {time_end - time_start}s\n")
	print(f"Training time: {time_end - time_start} seconds")  # Print the time taken by the fit function
	
	if LOAD_WEIGHTS:
		autoencoder.save_weights(PATH_WEIGHTS(True, N_EPOCHS, BATCH_SIZE, FILTERS))
	print("Training done, weights saved")
	plot_training_history(history, N_EPOCHS, name=f"conv{N_EPOCHS}_{BATCH_SIZE}_history")


# Calculate the reconstruction error for each image
error_reconstruction = np.mean(np.power(dataset - autoencoder.predict(dataset), 2), axis=1)

# Identify the 40 images with the highest reconstruction error as anomalies
anomalies = error_reconstruction.argsort()[-40:]

from sklearn.metrics import mean_squared_error

# Use the autoencoder to reconstruct the dataset
ds_reconstructed = autoencoder.predict(ds_preprocessed)

# Flatten the original and reconstructed datasets for comparison
ds_original_flattened = ds_preprocessed.reshape(ds_preprocessed.shape[0], -1)
ds_reconstructed_flattened = ds_reconstructed.reshape(ds_reconstructed.shape[0], -1)

# Calculate the reconstruction error for each image
error_reconstruction = np.array([mean_squared_error(ds_original_flattened[i], ds_reconstructed_flattened[i]) for i in range(ds_original_flattened.shape[0])])

error_reconstruction_flattened = error_reconstruction.flatten()
plot_hist_err_reconstruction(error_reconstruction_flattened, name=f"conv{N_EPOCHS}_{BATCH_SIZE}_hist")
plot_scatter_err_reconstruction(error_reconstruction, name=f"conv{N_EPOCHS}_{BATCH_SIZE}_scatter")

# Identify the 40 images with the highest reconstruction error as anomalies
anomalies = error_reconstruction.argsort()[-40:]

print(anomalies)
anomalies = sorted(anomalies)
anomalies_img = []
for i in anomalies:
	anomalies_img.append(dataset[i])
plot_images(anomalies_img, (8,5), name=f"conv{N_EPOCHS}_{BATCH_SIZE}")

"""
## Commenti

### Altri tentativi

autoencoder, normalized+flattened:
"""

#ds_preprocessed = (dataset / 255.0).reshape(dataset.shape[0], -1)
#
#input_img = Input(shape=(32*32*3,))
#encoded = Dense(128, activation='relu')(input_img)
#decoded = Dense(32*32*3, activation='sigmoid')(encoded)
#
#autoencoder = Model(input_img, decoded)
#autoencoder.compile(optimizer='adam', loss='binary_crossentropy')
#autoencoder.fit(ds_preprocessed, ds_preprocessed, epochs=50, batch_size=256, shuffle=True)
#


"""
Gli algoritmi di clustering non sono adatti al problema:
- KMeans e GMM non individuano anomalie
- DBSCAN e Ward non terminano l'esecuzione

E anche altre tecniche di clustering/unsupervised non sono adatte alle immagini:
- Isolation Forest
- One-Class SVM
- LOF
- elliptic

"""
