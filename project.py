# -*- coding: utf-8 -*-
"""project.ipynb

Automatically generated by Colab.

Original file is located at
	https://colab.research.google.com/drive/1K7v21oho6bkLGGm9fXiwG35X80FNMy4e

# Anomaly Detection in Cifar10

40 immagini deteriorate sono state aggiunte a una versione modificata del dataset cifar10.

TROVATELE!!

Istruzioni per scaricare il dataset sono date più in basso.
L'analisi deve fare riferimento solo al dataset fornito (non potete utilizzare in alcun modo Cifar10 originale).

## Cosa consegnare

Il risultato deve essere fornito sotto forma di **una lista di 40 indici** relativi agli outliers identificati.

Non potete restituire liste più lunghe. In tal caso, verrano troncate ai primi 40 elementi.

Il notebook deve spiegare la metodologia adottata e contenere le relative procedure. La metodologia deve essere automatica e non può prevedere nessuna supervisione umana.

## Dataset Downloading
"""

import numpy as np
import gdown

"""The following line should create in you local dicrectory a file named dataset.npy"""

# Use this line for Colab instead
#!gdown 1lccprYS7eWQBsLBsZS9J8qnETzRHBNZa
import os
if not os.path.exists('dataset.npy'):
	gdown.download('https://drive.google.com/uc?id=1lccprYS7eWQBsLBsZS9J8qnETzRHBNZa', 'dataset.npy', quiet=False)

dataset = np.load('dataset.npy',allow_pickle=True)

"""The dataset has shape (59900, 32, 32, 3)
No need to split it into train, validation and test.
"""

print(dataset.shape)

"""**IMPORTANTE**

Non riordinate il dataset caricato.
La lista di indici che restituite deve fare riferimento ad esso.


**Buon lavoro!!**

# Implementazione

Definizioni iniziali:
"""

import matplotlib.pyplot as plt

# True if running on Google Colab, False if in local
IN_COLAB = False

# True to allow loading saved weights
LOAD_WEIGHTS = False

INPUT_SIZE = 32*32*3	# 3072

if IN_COLAB:
	from google.colab import drive
	mount_path = '/content/drive'
	drive.mount(mount_path)
	save_path = mount_path +'/MyDrive/Colab Notebooks/res'
	PATH_WEIGHTS_AUTOENCODER	= f"{save_path}/weights_autoencoder.weights.h5"
	PATH_WEIGHTS_CONV			= f"{save_path}/weights_conv.weights.h5"
else:
	PATH_WEIGHTS_AUTOENCODER	= "weights_autoencoder.weights.h5"
	PATH_WEIGHTS_CONV			= "weights_conv.weights.h5"

PLOT_SHOW		= False
PLOT_COUNTER	= 0
def PATH_PLOT(name):
	global PLOT_COUNTER
	PLOT_COUNTER += 1
	return f"plot_{name}_{PLOT_COUNTER - 1}.png"


def img_show(img):
	plt.imshow(img, cmap="gray")
	plt.show()

def plot_images_horizontally(images, num_images, cmap='gray', name='horizontal'):
	plt.figure(figsize=(20, 20))
	for i in range(num_images):
		plt.subplot(1, num_images, i+1)
		plt.imshow(images[i], cmap=cmap)
		plt.axis('off')
	plt.savefig( PATH_PLOT(name) )
	if PLOT_SHOW:
		plt.show()

def plot_images(images, figsize, cmap='gray', name='grid'):
	scaler = 10
	plt.figure(figsize=(figsize[0]*scaler, figsize[1]*scaler))
	for i in range(min(figsize[0]*figsize[1], len(images))):
		plt.subplot(figsize[0], figsize[1], i+1)
		plt.imshow(images[i], cmap=cmap)
		plt.axis('off')
	plt.savefig( PATH_PLOT(name) )
	if PLOT_SHOW:
		plt.show()

def plot_hist(data, bins=50, name='hist'):
	plt.hist(data, bins=bins)
	plt.xlabel('Reconstruction error')
	plt.ylabel('No of examples')
	plt.title('Distribution of reconstruction errors')
	plt.savefig( PATH_PLOT(name) )
	if PLOT_SHOW:
		plt.show()


def plot_training_history(history, num_epochs, name='history'):

	training_loss = history.history['loss']
	training_metrics = history.history['accuracy']

	epochs = range(1, num_epochs + 1)
	plt.figure(figsize=(12, 4))

	plt.subplot(1, 2, 1)
	plt.plot(epochs, training_loss, label='Training Loss')
	plt.title('Loss')
	plt.xlabel('Epochs')
	plt.legend()

	plt.subplot(1, 2, 2)
	plt.plot(epochs, training_metrics, label='Training Accuracy')
	plt.title('Accuracy')
	plt.xlabel('Epochs')
	plt.legend()

	plt.tight_layout()
	plt.savefig( PATH_PLOT(name) )
	if PLOT_SHOW:
		plt.show()


#"""Gray scaling:"""
#
#x_gray = np.mean( dataset, axis=-1)
#
#print(f"{x_gray.shape = }")
#plot_images_horizontally(dataset[:10], 10, name="first10")
#plot_images_horizontally(x_gray[:10], 10, name="first10_gray")
#
"""Flattening:"""



"""Model (autoencoder):"""

# hypermarameters

BATCH_SIZE	= 256
N_EPOCHS	= 50
CONV_KERNEL_SIZE	= (1,1)
CONV_STRIDE			= (1,1)
MAX_POOLING_KERNEL_SIZE	= (2,2)
MAX_POOLING_STRIDE		= (2,2)


ds_preprocessed = (dataset / 255.0).reshape(dataset.shape[0], -1)

from keras.models import Model
from keras.layers import Input, Dense
from keras.layers import Conv2D, MaxPooling2D, UpSampling2D

# Define the autoencoder
input_img = Input(shape=(INPUT_SIZE,), name='input')
encoded = Dense(128, activation='relu', name='encoded')(input_img)
decoded = Dense(INPUT_SIZE, activation='sigmoid', name='decoded')(encoded)

autoencoder = Model(input_img, decoded)
autoencoder.summary()
autoencoder.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

# Train
try:
	if LOAD_WEIGHTS:
		autoencoder.load_weights(PATH_WEIGHTS_AUTOENCODER)
		print("Loaded weights for AUTOENCODER")
	else:
		raise Exception
except Exception as e:
	print(f"Loading AUTOENCODER Error: {e}")
	print("Didn't load weights, training AUTOENCODER")
	history = autoencoder.fit(ds_preprocessed, ds_preprocessed, epochs=N_EPOCHS, batch_size=BATCH_SIZE, shuffle=True)
	if LOAD_WEIGHTS:
		autoencoder.save_weights(PATH_WEIGHTS_AUTOENCODER)
	print("Training done, weights saved")
	plot_training_history(history, N_EPOCHS, name="autoencoder_history")


# Calculate the reconstruction error for each image
error_reconstruction = np.mean(np.power(ds_preprocessed - autoencoder.predict(ds_preprocessed), 2), axis=1)

plot_hist(error_reconstruction, name="autoencoder_hist")

# Identify the 40 images with the highest reconstruction error as anomalies
anomalies = error_reconstruction.argsort()[-40:]
anomalies = sorted(anomalies)

print(anomalies)
anomalies_img = []
for i in anomalies:
	anomalies_img.append(dataset[i])
plot_images(anomalies_img, (8,5), name="autoencoder")

"""not flattened (convolutional):"""

ds_preprocessed = dataset / 255.0

# Define the convolutional autoencoder
input_img = Input(shape=(32, 32, 3), name="input")
x = Conv2D(32, (3, 3), activation='relu', padding='same', name='encoded_conv2d_1')(input_img)
x = MaxPooling2D((2, 2), padding='same', name='encoded_maxPooling2d_1')(x)
x = Conv2D(32, (3, 3), activation='relu', padding='same', name='encoded_conv2d_2')(x)
encoded = MaxPooling2D((2, 2), padding='same', name='encoded_maxPooling2d_2')(x)

x = Conv2D(32, (3, 3), activation='relu', padding='same', name='decoded_conv2d_1')(encoded)
x = UpSampling2D((2, 2), name='decoded_upSampling2d_1')(x)
x = Conv2D(32, (3, 3), activation='relu', padding='same', name='decoded_conv2d_2')(x)
x = UpSampling2D((2, 2), name='decoded_upSampling2d_2')(x)
decoded = Conv2D(3, (3, 3), activation='sigmoid', padding='same', name='decoded_conv2d_3')(x)

autoencoder = Model(input_img, decoded)
autoencoder.summary()
autoencoder.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

# Train
try:
	if LOAD_WEIGHTS:
		autoencoder.load_weights(PATH_WEIGHTS_CONV)
		print("Loaded weights for CONVOLUTIONAL")
	else:
		raise Exception
except Exception as e:
	print(f"Loading CONVOLUTIONAL Error: {e}")
	print("Didn't load weights, training CONVOLUTIONAL")
	history = autoencoder.fit(ds_preprocessed, ds_preprocessed, epochs=50, batch_size=256, shuffle=True)
	if LOAD_WEIGHTS:
		autoencoder.save_weights(PATH_WEIGHTS_CONV)
	print("Training done, weights saved")
	plot_training_history(history, N_EPOCHS, name="conv_history")


# Calculate the reconstruction error for each image
error_reconstruction = np.mean(np.power(dataset - autoencoder.predict(dataset), 2), axis=1)

# Identify the 40 images with the highest reconstruction error as anomalies
anomalies = error_reconstruction.argsort()[-40:]

from sklearn.metrics import mean_squared_error

# Use the autoencoder to reconstruct the dataset
ds_reconstructed = autoencoder.predict(ds_preprocessed)

# Flatten the original and reconstructed datasets for comparison
ds_original_flattened = ds_preprocessed.reshape(ds_preprocessed.shape[0], -1)
ds_reconstructed_flattened = ds_reconstructed.reshape(ds_reconstructed.shape[0], -1)

# Calculate the reconstruction error for each image
error_reconstruction = np.array([mean_squared_error(ds_original_flattened[i], ds_reconstructed_flattened[i]) for i in range(ds_original_flattened.shape[0])])

error_reconstruction_flattened = error_reconstruction.flatten()
plot_hist(error_reconstruction_flattened, name="conv_hist")

# Identify the 40 images with the highest reconstruction error as anomalies
anomalies = error_reconstruction.argsort()[-40:]

print(anomalies)
anomalies = sorted(anomalies)
anomalies_img = []
for i in anomalies:
	anomalies_img.append(dataset[i])
plot_images(anomalies_img, (8,5), name="convolutional")

"""
## Commenti

### Altri tentativi

autoencoder, normalized+flattened:
"""

#ds_preprocessed = (dataset / 255.0).reshape(dataset.shape[0], -1)
#
#input_img = Input(shape=(32*32*3,))
#encoded = Dense(128, activation='relu')(input_img)
#decoded = Dense(32*32*3, activation='sigmoid')(encoded)
#
#autoencoder = Model(input_img, decoded)
#autoencoder.compile(optimizer='adam', loss='binary_crossentropy')
#autoencoder.fit(ds_preprocessed, ds_preprocessed, epochs=50, batch_size=256, shuffle=True)
#



